{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c86fbc-20d2-422a-be9a-44bd53ae126d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 19:17:09.765823: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-27 19:17:09.765901: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-27 19:17:09.765987: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # must come BEFORE TensorFlow\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # optional: suppress logs\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import albumentations as A\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "import glob\n",
    "from tensorflow.keras.mixed_precision import Policy as DTypePolicy\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "\n",
    "\n",
    "# 1. Load your trained model and tools (EXACTLY like your unit test)\n",
    "# model = load_model(\"/home/ayombalima/ml_models/student_recognition_model.h5\")\n",
    "class CustomInputLayer(InputLayer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        bs = kwargs.pop(\"batch_shape\", None)\n",
    "        if bs is not None:\n",
    "            kwargs[\"batch_input_shape\"] = bs\n",
    "        super().__init__(*args, **kwargs)\n",
    "ML_MODEL_PATH = \"/home/ayombalima/ml_models/student_recognition_model.h5\"\n",
    "model     = load_model(\n",
    "    ML_MODEL_PATH,\n",
    "    compile=False,\n",
    "    safe_mode=False,\n",
    "    custom_objects={\"InputLayer\": CustomInputLayer, \"DTypePolicy\": DTypePolicy}\n",
    ")\n",
    "le = joblib.load(\"/home/ayombalima/ml_models/label_encoder.pkl\")\n",
    "scaler = joblib.load(\"/home/ayombalima/ml_models/scaler.pkl\")\n",
    "\n",
    "with open(\"/home/ayombalima/ml_models/final_clustered_results.json\", \"r\") as f:\n",
    "    clustered_data = json.load(f)\n",
    "\n",
    "id_to_path = {\n",
    "    v: k for k, v in clustered_data[\"student_id_mapping\"].items()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 2. Same augmenter used for Mariam\n",
    "augmenter = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=15, p=0.6),\n",
    "    A.RandomBrightnessContrast(p=0.6),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "    A.RandomShadow(p=0.2)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571e5678-fac6-4bc8-85a2-b7472efb1cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. YOLO Detection (EXACTLY like your command)\n",
    "def run_yolo_detection(source):\n",
    "    os.system(f\"\"\"\n",
    "    /home/ayombalima/backend_env/bin/python /home/ayombalima/YOLO-FaceV2-master/detect.py \\\n",
    "        --weights /home/ayombalima/YOLO-FaceV2-master/yolov5s_v2.pt \\\n",
    "        --img 640 \\\n",
    "        --conf 0.25 \\\n",
    "        --source {source} \\\n",
    "        --save-txt --save-conf\n",
    "    \"\"\")\n",
    "    detect_folders = glob.glob(\"runs/detect/exp*\")\n",
    "    return max(detect_folders, key=os.path.getctime)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d52892-55a2-4511-be3a-5df979073e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate embeddings (IDENTICAL to Mariam's approach)\n",
    "def generate_embeddings(face_img):\n",
    "    embeddings = []\n",
    "    try:\n",
    "        # Original\n",
    "        original_emb = DeepFace.represent(face_img, model_name=\"Facenet\", enforce_detection=False)[0][\"embedding\"]\n",
    "        embeddings.append(original_emb)\n",
    "        \n",
    "        # 9 augmentations\n",
    "        for _ in range(9):\n",
    "            aug_img = augmenter(image=face_img)['image']\n",
    "            aug_emb = DeepFace.represent(aug_img, model_name=\"Facenet\", enforce_detection=False)[0][\"embedding\"]\n",
    "            embeddings.append(aug_emb)\n",
    "    except:\n",
    "        pass\n",
    "    return np.array(embeddings) if embeddings else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbbf0471-340f-4d6b-85ad-b8d48f63a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Identify student using your model\n",
    "# def identify_student(embeddings):\n",
    "#     if embeddings is None: \n",
    "#         return \"Unknown\", 0.0\n",
    "#     avg_embed = np.mean(embeddings, axis=0).reshape(1, -1)\n",
    "#     scaled = scaler.transform(avg_embed)\n",
    "#     pred_probs = model.predict(scaled)[0]\n",
    "#     return le.inverse_transform([np.argmax(pred_probs)])[0], float(np.max(pred_probs))\n",
    "\n",
    "def identify_student(embeddings):\n",
    "    if embeddings is None: \n",
    "        return \"Unknown\", 0.0, None\n",
    "    avg_embed = np.mean(embeddings, axis=0).reshape(1, -1)\n",
    "    scaled = scaler.transform(avg_embed)\n",
    "    pred_probs = model.predict(scaled)[0]\n",
    "    pred_id = le.inverse_transform([np.argmax(pred_probs)])[0]\n",
    "    original_source = id_to_path.get(pred_id, \"N/A\")\n",
    "    return pred_id, float(np.max(pred_probs)), original_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571913a2-0c97-41c5-a26a-cd460da9c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6. MAIN VIDEO PROCESSING FUNCTION\n",
    "# def process_video(video_path, output_dir=\"video_results\"):\n",
    "#     # Create output dir\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # Extract frames at 2 FPS\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     frame_count = 0\n",
    "#     saved_frames = []\n",
    "    \n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret: break\n",
    "        \n",
    "#         frame_count += 1\n",
    "#         if frame_count % int(cap.get(cv2.CAP_PROP_FPS)/2) != 0:  # 2 FPS\n",
    "#             continue\n",
    "            \n",
    "#         frame_path = os.path.join(output_dir, f\"frame_{frame_count:05d}.jpg\")\n",
    "#         cv2.imwrite(frame_path, frame)\n",
    "#         saved_frames.append(frame_path)\n",
    "    \n",
    "#     cap.release()\n",
    "#     print(f\"Extracted {len(saved_frames)} frames\")\n",
    "    \n",
    "#     # Process frames with YOLO\n",
    "#     detect_dir = run_yolo_detection(output_dir)\n",
    "#     labels_dir = os.path.join(detect_dir, \"labels\")\n",
    "    \n",
    "#     # Process detections\n",
    "#     results = []\n",
    "#     for frame_path in saved_frames:\n",
    "#         frame_name = os.path.basename(frame_path).split('.')[0]\n",
    "#         label_path = os.path.join(labels_dir, f\"{frame_name}.txt\")\n",
    "        \n",
    "#         if not os.path.exists(label_path):\n",
    "#             continue\n",
    "            \n",
    "#         # Read YOLO detections\n",
    "#         with open(label_path, 'r') as f:\n",
    "#             detections = [list(map(float, line.strip().split())) for line in f.readlines()]\n",
    "        \n",
    "#         # Process each face\n",
    "#         frame = cv2.imread(frame_path)\n",
    "#         h, w = frame.shape[:2]\n",
    "        \n",
    "#         for det in detections:\n",
    "#             # Convert YOLO to pixels (EXACTLY like your Mariam code)\n",
    "#             x_center, y_center, width, height = det[1:5]\n",
    "#             x1 = int((x_center - width/2) * w)\n",
    "#             y1 = int((y_center - height/2) * h)\n",
    "#             x2 = int((x_center + width/2) * w)\n",
    "#             y2 = int((y_center + height/2) * h)\n",
    "            \n",
    "#             # Extract and process face\n",
    "#             face_img = frame[y1:y2, x1:x2]\n",
    "#             if face_img.size == 0: continue\n",
    "            \n",
    "#             # Generate embeddings (SAME as Mariam)\n",
    "#             face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "#             embeddings = generate_embeddings(face_rgb)\n",
    "            \n",
    "#             # Identify student\n",
    "#             name, confidence, source_file = identify_student(embeddings)\n",
    "#             print(f\"✅ Frame: {frame_path} | Predicted: {name} ({confidence:.2f}) from source file: {source_file}\")\n",
    "\n",
    "#             # if confidence >= 0.80:\n",
    "#             #     print(f\"✅ Frame: {frame_path} | Predicted: {name} ({confidence:.2f}) from source file: {source_file}\")\n",
    "            \n",
    "#             # Save results\n",
    "#             results.append({\n",
    "#                 \"frame\": frame_path,\n",
    "#                 \"student\": name,\n",
    "#                 \"confidence\": confidence,\n",
    "#                 \"source_file\": source_file,\n",
    "#                 \"bbox\": [x1, y1, x2, y2]\n",
    "#             })\n",
    "            \n",
    "#             # Draw on frame\n",
    "#             color = (0, 255, 0) if name != \"Unknown\" else (0, 0, 255)\n",
    "#             cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "#             cv2.putText(frame, f\"{name} ({confidence:.2f})\", (x1, y1-10), \n",
    "#                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "#             # Save annotated frame\n",
    "#             cv2.imwrite(frame_path, frame)\n",
    "    \n",
    "#     # Save all results\n",
    "#     with open(os.path.join(output_dir, \"results.json\"), 'w') as f:\n",
    "#         json.dump(results, f, indent=2)\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fff0d57-b1d8-4f5f-bdf4-2916d2718ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_dir=\"video_results\"):\n",
    "    import cv2\n",
    "    import os\n",
    "    import json\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Create output dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract frames at 2 FPS\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    saved_frames = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % int(cap.get(cv2.CAP_PROP_FPS)/2) != 0:  # 2 FPS\n",
    "            continue\n",
    "            \n",
    "        frame_path = os.path.join(output_dir, f\"frame_{frame_count:05d}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        saved_frames.append(frame_path)\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Extracted {len(saved_frames)} frames\")\n",
    "    \n",
    "    # Process frames with YOLO\n",
    "    detect_dir = run_yolo_detection(output_dir)\n",
    "    labels_dir = os.path.join(detect_dir, \"labels\")\n",
    "    \n",
    "    results = []\n",
    "    for frame_path in saved_frames:\n",
    "        frame_name = os.path.basename(frame_path).split('.')[0]\n",
    "        label_path = os.path.join(labels_dir, f\"{frame_name}.txt\")\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "            \n",
    "        # Read YOLO detections\n",
    "        with open(label_path, 'r') as f:\n",
    "            detections = [list(map(float, line.strip().split())) for line in f.readlines()]\n",
    "        \n",
    "        frame = cv2.imread(frame_path)\n",
    "        h, w = frame.shape[:2]\n",
    "        \n",
    "        for det in detections:\n",
    "            x_center, y_center, width, height = det[1:5]\n",
    "            x1 = int((x_center - width/2) * w)\n",
    "            y1 = int((y_center - height/2) * h)\n",
    "            x2 = int((x_center + width/2) * w)\n",
    "            y2 = int((y_center + height/2) * h)\n",
    "            \n",
    "            face_img = frame[y1:y2, x1:x2]\n",
    "            if face_img.size == 0:\n",
    "                continue\n",
    "            \n",
    "            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "            embeddings = generate_embeddings(face_rgb)\n",
    "            \n",
    "            name, confidence, source_file = identify_student(embeddings)\n",
    "\n",
    "            if confidence < 0.80:\n",
    "                name = \"Unknown\"\n",
    "\n",
    "            results.append({\n",
    "                \"frame\": frame_path,\n",
    "                \"student\": name,\n",
    "                \"confidence\": confidence,\n",
    "                \"source_file\": source_file,\n",
    "                \"bbox\": [x1, y1, x2, y2]\n",
    "            })\n",
    "\n",
    "            if name == \"Unknown\":\n",
    "                print(f\"❌ Frame: {frame_path} | Prediction below threshold - Marked as Unknown\")\n",
    "            else:\n",
    "                print(f\"✅ Frame: {frame_path} | Predicted: {name} ({confidence:.2f}) from source file: {source_file}\")\n",
    "            \n",
    "            color = (0, 255, 0) if name != \"Unknown\" else (0, 0, 255)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            label = name if name != \"Unknown\" else \"Unknown\"\n",
    "            cv2.putText(frame, f\"{label} ({confidence:.2f})\", (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "\n",
    "    # Save results\n",
    "    with open(os.path.join(output_dir, \"results.json\"), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    # === CALL SUMMARY HERE BEFORE RETURN ===\n",
    "    summarize_final_identifications(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def summarize_final_identifications(results):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    student_summary = defaultdict(lambda: {\"source_file\": None, \"max_confidence\": 0.0, \"count\": 0})\n",
    "\n",
    "    for result in results:\n",
    "        student = result[\"student\"]\n",
    "        if student == \"Unknown\":\n",
    "            continue  # Skip unknowns from summary\n",
    "        \n",
    "        conf = result[\"confidence\"]\n",
    "        source = result[\"source_file\"]\n",
    "\n",
    "        if conf > student_summary[student][\"max_confidence\"]:\n",
    "            student_summary[student][\"max_confidence\"] = conf\n",
    "            student_summary[student][\"source_file\"] = source\n",
    "        \n",
    "        student_summary[student][\"count\"] += 1\n",
    "\n",
    "    if not student_summary:\n",
    "        print(\"\\n Final Identification Results: None found.\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n Final Identification Results:\\n\")\n",
    "    for idx, (student, info) in enumerate(student_summary.items()):\n",
    "        print(f\"- Student_{idx}: {info['source_file']}.jpg (confidence: {info['max_confidence']:.2f}, seen in {info['count']} frames)\")\n",
    "\n",
    "# def summarize_final_identifications(results):\n",
    "#     from collections import defaultdict\n",
    "\n",
    "#     student_summary = defaultdict(lambda: {\"source_file\": None, \"max_confidence\": 0.0, \"count\": 0})\n",
    "\n",
    "#     for result in results:\n",
    "#         student = result[\"student\"]\n",
    "#         if student == \"Unknown\":\n",
    "#             continue\n",
    "        \n",
    "#         conf = result[\"confidence\"]\n",
    "#         source = result[\"source_file\"]\n",
    "\n",
    "#         if conf > student_summary[student][\"max_confidence\"]:\n",
    "#             student_summary[student][\"max_confidence\"] = conf\n",
    "#             student_summary[student][\"source_file\"] = source\n",
    "        \n",
    "#         student_summary[student][\"count\"] += 1\n",
    "\n",
    "#     if not student_summary:\n",
    "#         print(\"\\n🎯 Final Identification Results: None found.\\n\")\n",
    "#         return\n",
    "\n",
    "#     # 👉 NEW FILTER: Accept students with max_confidence >= 0.80\n",
    "#     filtered_students = {student: info for student, info in student_summary.items() if info[\"max_confidence\"] >= 0.80}\n",
    "\n",
    "#     if not filtered_students:\n",
    "#         print(\"\\n🎯 Final Identification Results: None passed filtering (≥ 80% confidence).\\n\")\n",
    "#         return\n",
    "\n",
    "#     # Sort by highest confidence among accepted\n",
    "#     sorted_students = sorted(filtered_students.items(), key=lambda x: x[1][\"max_confidence\"], reverse=True)\n",
    "\n",
    "#     print(\"\\n🎯 Final Identification Results:\\n\")\n",
    "#     for idx, (student, info) in enumerate(sorted_students):\n",
    "#         print(f\"- Student_{idx}: {info['source_file']}.jpg (confidence: {info['max_confidence']:.2f}, seen in {info['count']} frames)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d07dea-0912-4574-9ba3-05f8988dfe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS THE ONE CHAT GAVE WHICH IS THE EDITED VERSION OF THE FIRST COMMENTED OUT, \n",
    "#THE ONE ABOVE THIS ONE IS FOR THE PREDICTIONS MATCHING THE NUMBER OF FACES AND ONLY DISPLAYING IF CONFIDENCE IS > 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36bcf9d7-e30e-409d-b6e7-2eae42b56481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # 6. MAIN VIDEO PROCESSING FUNCTION\n",
    "# def process_video(video_path, output_dir=\"video_results\"):\n",
    "#     import cv2\n",
    "#     import os\n",
    "#     import json\n",
    "#     from collections import defaultdict\n",
    "\n",
    "#     # Create output dir\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # Extract frames at 2 FPS\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     frame_count = 0\n",
    "#     saved_frames = []\n",
    "    \n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "        \n",
    "#         frame_count += 1\n",
    "#         if frame_count % int(cap.get(cv2.CAP_PROP_FPS)/2) != 0:  # 2 FPS\n",
    "#             continue\n",
    "            \n",
    "#         frame_path = os.path.join(output_dir, f\"frame_{frame_count:05d}.jpg\")\n",
    "#         cv2.imwrite(frame_path, frame)\n",
    "#         saved_frames.append(frame_path)\n",
    "    \n",
    "#     cap.release()\n",
    "#     print(f\"Extracted {len(saved_frames)} frames\")\n",
    "    \n",
    "#     # Process frames with YOLO\n",
    "#     detect_dir = run_yolo_detection(output_dir)\n",
    "#     labels_dir = os.path.join(detect_dir, \"labels\")\n",
    "    \n",
    "#     # Process detections\n",
    "#     results = []\n",
    "#     for frame_path in saved_frames:\n",
    "#         frame_name = os.path.basename(frame_path).split('.')[0]\n",
    "#         label_path = os.path.join(labels_dir, f\"{frame_name}.txt\")\n",
    "        \n",
    "#         if not os.path.exists(label_path):\n",
    "#             continue\n",
    "            \n",
    "#         # Read YOLO detections\n",
    "#         with open(label_path, 'r') as f:\n",
    "#             detections = [list(map(float, line.strip().split())) for line in f.readlines()]\n",
    "        \n",
    "#         # Process each face\n",
    "#         frame = cv2.imread(frame_path)\n",
    "#         h, w = frame.shape[:2]\n",
    "        \n",
    "#         for det in detections:\n",
    "#             # Convert YOLO to pixel coordinates\n",
    "#             x_center, y_center, width, height = det[1:5]\n",
    "#             x1 = int((x_center - width/2) * w)\n",
    "#             y1 = int((y_center - height/2) * h)\n",
    "#             x2 = int((x_center + width/2) * w)\n",
    "#             y2 = int((y_center + height/2) * h)\n",
    "            \n",
    "#             # Extract and process face\n",
    "#             face_img = frame[y1:y2, x1:x2]\n",
    "#             if face_img.size == 0:\n",
    "#                 continue\n",
    "            \n",
    "#             face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "#             embeddings = generate_embeddings(face_rgb)\n",
    "            \n",
    "#             # Predict identity\n",
    "#             name, confidence, source_file = identify_student(embeddings)\n",
    "#             print(f\"✅ Frame: {frame_path} | Predicted: {name} ({confidence:.2f}) from source file: {source_file}\")\n",
    "            \n",
    "#             # Save results\n",
    "#             results.append({\n",
    "#                 \"frame\": frame_path,\n",
    "#                 \"student\": name,\n",
    "#                 \"confidence\": confidence,\n",
    "#                 \"source_file\": source_file,\n",
    "#                 \"bbox\": [x1, y1, x2, y2]\n",
    "#             })\n",
    "            \n",
    "#             # Annotate frame\n",
    "#             color = (0, 255, 0) if name != \"Unknown\" else (0, 0, 255)\n",
    "#             cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "#             cv2.putText(frame, f\"{name} ({confidence:.2f})\", (x1, y1-10), \n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "#             cv2.imwrite(frame_path, frame)\n",
    "\n",
    "#     # === Post-process results: Filter based on frequency and confidence ===\n",
    "\n",
    "#     # Step 1: Group predictions by student ID\n",
    "#     student_hits = defaultdict(list)\n",
    "#     for result in results:\n",
    "#         sid = result[\"student\"]\n",
    "#         conf = result[\"confidence\"]\n",
    "#         if sid != \"Unknown\":\n",
    "#             student_hits[sid].append(conf)\n",
    "\n",
    "#     # Step 2: Decide valid IDs — at least 2 hits and one ≥ 0.8\n",
    "#     valid_ids = set()\n",
    "#     for sid, confs in student_hits.items():\n",
    "#         if len(confs) >= 2 and any(c >= 0.8 for c in confs):\n",
    "#             valid_ids.add(sid)\n",
    "\n",
    "#     # Step 3: Rewrite results based on the valid IDs\n",
    "#     for r in results:\n",
    "#         if r[\"student\"] not in valid_ids:\n",
    "#             r[\"student\"] = \"Unknown\"\n",
    "\n",
    "#     # Step 4: Save results\n",
    "#     with open(os.path.join(output_dir, \"results.json\"), 'w') as f:\n",
    "#         json.dump(results, f, indent=2)\n",
    "\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca9c4033-2316-4d49-827e-4d42ecb06d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 13 frames\n",
      "Namespace(weights=['/home/ayombalima/YOLO-FaceV2-master/yolov5s_v2.pt'], source='video_results', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, plot_label=False, save_txt=True, save_conf=True, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /home/ayombalima/backend/tasks/requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 2024-2-11 torch 2.6.0+cu124 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "image 1/23 /home/ayombalima/backend/tasks/video_results/frame_00014.jpg: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 394 layers, 18706966 parameters, 0 gradients\n",
      "/home/ayombalima/backend_env/lib/python3.10/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384x640 Done. (0.420s)\n",
      "image 2/23 /home/ayombalima/backend/tasks/video_results/frame_00028.jpg: 384x640 Done. (0.297s)\n",
      "image 3/23 /home/ayombalima/backend/tasks/video_results/frame_00042.jpg: 384x640 Done. (0.311s)\n",
      "image 4/23 /home/ayombalima/backend/tasks/video_results/frame_00056.jpg: 384x640 Done. (0.324s)\n",
      "image 5/23 /home/ayombalima/backend/tasks/video_results/frame_00070.jpg: 384x640 1 face, Done. (0.329s)\n",
      "image 6/23 /home/ayombalima/backend/tasks/video_results/frame_00084.jpg: 384x640 2 faces, Done. (0.301s)\n",
      "image 7/23 /home/ayombalima/backend/tasks/video_results/frame_00098.jpg: 384x640 2 faces, Done. (0.290s)\n",
      "image 8/23 /home/ayombalima/backend/tasks/video_results/frame_00112.jpg: 384x640 2 faces, Done. (0.291s)\n",
      "image 9/23 /home/ayombalima/backend/tasks/video_results/frame_00126.jpg: 384x640 2 faces, Done. (0.264s)\n",
      "image 10/23 /home/ayombalima/backend/tasks/video_results/frame_00140.jpg: 384x640 2 faces, Done. (0.299s)\n",
      "image 11/23 /home/ayombalima/backend/tasks/video_results/frame_00154.jpg: 384x640 2 faces, Done. (0.274s)\n",
      "image 12/23 /home/ayombalima/backend/tasks/video_results/frame_00168.jpg: 384x640 2 faces, Done. (0.303s)\n",
      "image 13/23 /home/ayombalima/backend/tasks/video_results/frame_00182.jpg: 384x640 2 faces, Done. (0.291s)\n",
      "image 14/23 /home/ayombalima/backend/tasks/video_results/frame_00196.jpg: 384x640 2 faces, Done. (0.292s)\n",
      "image 15/23 /home/ayombalima/backend/tasks/video_results/frame_00210.jpg: 384x640 1 face, Done. (0.284s)\n",
      "image 16/23 /home/ayombalima/backend/tasks/video_results/frame_00224.jpg: 384x640 2 faces, Done. (0.292s)\n",
      "image 17/23 /home/ayombalima/backend/tasks/video_results/frame_00238.jpg: 384x640 1 face, Done. (0.289s)\n",
      "image 18/23 /home/ayombalima/backend/tasks/video_results/frame_00252.jpg: 384x640 2 faces, Done. (0.294s)\n",
      "image 19/23 /home/ayombalima/backend/tasks/video_results/frame_00266.jpg: 384x640 2 faces, Done. (0.298s)\n",
      "image 20/23 /home/ayombalima/backend/tasks/video_results/frame_00280.jpg: 384x640 2 faces, Done. (0.279s)\n",
      "image 21/23 /home/ayombalima/backend/tasks/video_results/frame_00294.jpg: 384x640 1 face, Done. (0.288s)\n",
      "image 22/23 /home/ayombalima/backend/tasks/video_results/frame_00308.jpg: 384x640 2 faces, Done. (0.296s)\n",
      "image 23/23 /home/ayombalima/backend/tasks/video_results/frame_00322.jpg: 384x640 2 faces, Done. (0.301s)\n",
      "Results saved to runs/detect/exp9\n",
      "19 labels saved to runs/detect/exp9/labels\n",
      "Done. (7.209s)\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "✅ Frame: video_results/frame_00070.jpg | Predicted: student_86 (0.87) from source file: s15_face_12\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "❌ Frame: video_results/frame_00084.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "❌ Frame: video_results/frame_00084.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "❌ Frame: video_results/frame_00098.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "❌ Frame: video_results/frame_00098.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "❌ Frame: video_results/frame_00112.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "❌ Frame: video_results/frame_00112.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "❌ Frame: video_results/frame_00126.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "❌ Frame: video_results/frame_00126.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "❌ Frame: video_results/frame_00140.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "❌ Frame: video_results/frame_00140.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "❌ Frame: video_results/frame_00154.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "❌ Frame: video_results/frame_00154.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "❌ Frame: video_results/frame_00168.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "❌ Frame: video_results/frame_00168.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "❌ Frame: video_results/frame_00182.jpg | Prediction below threshold - Marked as Unknown\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "❌ Frame: video_results/frame_00182.jpg | Prediction below threshold - Marked as Unknown\n",
      "\n",
      " Final Identification Results:\n",
      "\n",
      "- Student_0: s15_face_12.jpg (confidence: 0.87, seen in 1 frames)\n"
     ]
    }
   ],
   "source": [
    "# Testing Video Upload with gathered sample\n",
    "VIDEO_PATH = \"/home/ayombalima/video_uploads/VID-20221017-WA0003.mp4\" \n",
    "results = process_video(VIDEO_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c607ac-3637-4986-9a57-f5c3ed67b298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33cba13-e26a-4be9-829f-58dd75c8960d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f1859-69da-44fd-b8b2-616c7d3dadca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61949441-0989-48d5-a642-290c9c7905ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80dce12-6eed-472a-9fc4-bfbbf8e756e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09915613-8391-4f13-99e4-5b3e9bfc8a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab21e5-031c-49c0-93c4-a4d99f911169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb77eb7-6a8e-4e24-bef8-3a5aa73855fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df66fb9-5ec2-4e19-987c-643e3eec79cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
